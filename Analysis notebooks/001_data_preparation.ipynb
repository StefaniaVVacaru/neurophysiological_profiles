{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "### Purpose\n",
    "Purpose of this notebook is to get the Mindware data (signals & events) ready for further processing.\n",
    "\n",
    "### Approach\n",
    "1. Load the signal data and the event data per subject\n",
    "2. If there are > 1 signal / event data, load both\n",
    "3. In the event data, assign event onset and event offset for each event. Every event minimally has an onset\n",
    "4. Join the event data with the signal data and concatenate in case of > 1 event/signal data\n",
    "### Input / Output\n",
    "\n",
    "- Input `~/data/raw/signals` & `~/data/raw/events`\n",
    "- Output `~/data/interim/signals` & `~/data/interim/events`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ecg_utils.common' from '/Users/lukasspiess/Library/CloudStorage/OneDrive-SpiessSolution/Neurophysiological profiles/General/Mindware data analysis/src/ecg_utils/common.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fmt: off\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Dict\n",
    "import sys\n",
    "import pandas as pd \n",
    "import importlib\n",
    "sys.path.append(str(Path().cwd().parent/\"src\"))\n",
    "import ecg_utils.data_utils as data_utils\n",
    "import ecg_utils.parameters as parameters\n",
    "import ecg_utils.common as common\n",
    "import numpy as np\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(parameters)\n",
    "importlib.reload(common)\n",
    "# fmt:on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = Path().cwd()\n",
    "ROOT_DIR = WORKING_DIR.parent\n",
    "\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "\n",
    "RAW_SIGNAL_DATA_DIR = DATA_DIR / 'raw' / 'signals'\n",
    "RAW_EVENT_DATA_DIR = DATA_DIR / 'raw' / 'events'\n",
    "\n",
    "INTERIM_SIGNAL_DATA_DIR = DATA_DIR / 'interim' / 'signals'\n",
    "INTERIM_EVENT_DATA_DIR = DATA_DIR / 'interim' / 'events'\n",
    "\n",
    "INTERIM_SIGNAL_DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "INTERIM_EVENT_DATA_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some participants, recording has stopped and was continued again. This resulted in having two (or more) data files per participant. This is true for both the raw signal data and for the event data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_frequency = parameters.base_params['general'].get(\"sampling_frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get all unique subject ids from the list of files\n",
    "\"\"\"\n",
    "all_subject_ids = set()\n",
    "data_filepaths = list(RAW_SIGNAL_DATA_DIR.glob('*.txt'))\n",
    "\n",
    "for flp in data_filepaths:\n",
    "    filename = flp.stem\n",
    "    if not (filename.lower().startswith(\"asamb\")) and filename[:4].isnumeric():\n",
    "        sub_id_str = filename.split(\"_\")[0]\n",
    "        all_subject_ids.add(sub_id_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject 2114\n",
      "Processing subject 2108\n",
      "Processing subject 2103\n",
      "Processing subject 2115\n",
      "Processing subject 2105\n",
      "Processing subject 2112\n",
      "Processing subject 2113\n",
      "Processing subject 2110\n",
      "Processing subject 2104\n",
      "Processing subject 2100\n",
      "Processing subject 2109\n",
      "Processing subject 2102\n",
      "Processing subject 2111\n",
      "Processing subject 2106\n",
      "Processing subject 2107\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read data and event files, join them, and export as single file\n",
    "\"\"\"\n",
    "for subject_id in all_subject_ids:\n",
    "    print(f\"Processing subject {subject_id}\")\n",
    "    # if subject_id != \"2084\":\n",
    "    #     continue\n",
    "    # retrieve filepaths of signal and event data and assure they match in number\n",
    "    data_filepaths = list(RAW_SIGNAL_DATA_DIR.glob(f\"{subject_id}*\"))\n",
    "    event_filepaths = list(RAW_EVENT_DATA_DIR.glob(f\"ASAMB_{subject_id}*\"))\n",
    "    if len(data_filepaths) not in (1,2):\n",
    "        raise ValueError(f\"Expected 1 or 2 data files for subject {subject_id}. Got {len(data_filepaths)}\")\n",
    "    if len(data_filepaths) != len(event_filepaths):\n",
    "        event_filepaths = list(RAW_EVENT_DATA_DIR.glob(f\"asamb_{subject_id}*\"))\n",
    "    if len(data_filepaths) != len(event_filepaths):\n",
    "        raise ValueError(f\"Did not find the same number of data files and event files for subject {subject_id}. Got {len(data_filepaths)} and {len(event_filepaths)}\")\n",
    "    \n",
    "    # Read the signal and event data\n",
    "    data_df_list = [pd.read_csv(RAW_SIGNAL_DATA_DIR / flp, delimiter=\"\\t\", skiprows=1).assign(source_file = str(flp.parts[-1])) for flp in np.sort(data_filepaths)]\n",
    "    event_df_list = [pd.read_csv(RAW_EVENT_DATA_DIR / flp, delimiter=\"\\t\").assign(source_file = str(flp.parts[-1])) for flp in np.sort(event_filepaths)]\n",
    "    \n",
    "    \n",
    "    # Preprocess the event data\n",
    "    try:\n",
    "        event_df_list = [data_utils.preprocess_event_data(df) for df in event_df_list]\n",
    "    except Exception as e:\n",
    "        print(f\"The following error occurred when processing subject {subject_id}: {e}\")\n",
    "    \n",
    "    # Left-Join the signal data with the events data\n",
    "    merged_df_list = []\n",
    "    for signal_df, event_df in zip(data_df_list, event_df_list):\n",
    "        event_df[\"Time\"] = event_df[\"Time\"].apply(common.comma_str_2_float)\n",
    "        signal_df['Time (s)'] = signal_df['Time (s)'].apply(lambda x: common.comma_str_2_float(x) if isinstance(x, str) else x)\n",
    "        merged_df_list.append(\n",
    "            signal_df.merge(\n",
    "                event_df,\n",
    "                how = 'left',\n",
    "                left_on = ['Time (s)'],\n",
    "                right_on = ['Time'],\n",
    "                suffixes=('_signal', '_event')\n",
    "            ).drop(columns = [\"Time\"])\n",
    "        )\n",
    "    \n",
    "    # concatenate and save\n",
    "    signal_df = pd.concat(merged_df_list)\n",
    "    signal_df = (\n",
    "        signal_df\n",
    "        .assign(subject_id = subject_id, row_index = range(len(signal_df)))\n",
    "        .rename(columns = {\"Name\": \"event_name\", \"Time (s)\": \"time_seconds_original_file\"})\n",
    "        .set_index(\"row_index\", drop = True)\n",
    "        )\n",
    "    signal_df.index.name = \"row_index\"\n",
    "    signal_df.to_csv(INTERIM_SIGNAL_DATA_DIR/f\"{subject_id}_signal_events.csv\", index = False)\n",
    "    signal_df[~pd.isnull(signal_df[\"event_name\"])].to_excel(INTERIM_EVENT_DATA_DIR/f\"{subject_id}_events.xlsx\", index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroprofile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
