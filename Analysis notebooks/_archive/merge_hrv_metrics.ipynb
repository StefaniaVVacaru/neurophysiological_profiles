{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge HRV Metrics\n",
    "\n",
    "Purpose of this notebook is to load all the calculate HRV metrics from the Excel sheets and to concatenate them into a single Excel sheet. This facilitates data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = Path().cwd()\n",
    "ROOT_DIR = WORKING_DIR.parent\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "HRV_DATA_DIR = DATA_DIR / 'hrv' # the output directory for the concatenated hrv data\n",
    "HRV_DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "REPORTS_DIR = ROOT_DIR / 'reports'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plausible_to_nan(\n",
    "    df: pd.DataFrame, \n",
    "    column: str, \n",
    "    lower_bound: float = 9, \n",
    "    upper_bound: float = 110\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Set implausible values in a specified DataFrame column to NaN based on user-defined bounds.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing the data to process.\n",
    "    column : str\n",
    "        The name of the column to check for plausibility.\n",
    "    lower_bound : float, optional\n",
    "        The lower bound for plausible values. Values below this will be set to NaN. Default is 9.\n",
    "    upper_bound : float, optional\n",
    "        The upper bound for plausible values. Values above this will be set to NaN. Default is 110.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with implausible values in the specified column replaced by NaN.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # To avoid modifying the original DataFrame\n",
    "    df[column] = np.where(\n",
    "        (df[column] < lower_bound) | (df[column] > upper_bound), \n",
    "        np.nan, \n",
    "        df[column]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def identify_outliers_zscore(\n",
    "    data: Union[List[float], np.ndarray, pd.Series], \n",
    "    threshold: float = 1.96\n",
    ") -> List[bool]:\n",
    "    \"\"\"\n",
    "    Identify outliers in a dataset based on the Z-score method.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : Union[List[float], np.ndarray, pd.Series]\n",
    "        The input data, which can be a list, numpy array, or pandas series.\n",
    "    threshold : float, optional\n",
    "        The Z-score threshold to determine outliers. Default is 1.96 (i.e., >95%).\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    List[bool]\n",
    "        A list of booleans indicating whether each value is an outlier (True) or not (False).\n",
    "        NaN values are treated as outliers.\n",
    "    \"\"\"\n",
    "    # Convert data to a numpy array\n",
    "    data_array = np.asarray(data, dtype=np.float64)\n",
    "    \n",
    "    # Identify NaN values (NaN will be treated as outliers)\n",
    "    nan_mask = np.isnan(data_array)\n",
    "    \n",
    "    # Calculate Z-scores only for non-NaN values\n",
    "    valid_data = data_array[~nan_mask]\n",
    "    mean = valid_data.mean()\n",
    "    std = valid_data.std(ddof=0)\n",
    "    \n",
    "    z_scores = (valid_data - mean) / std\n",
    "    outliers_non_nan = np.abs(z_scores) > threshold\n",
    "    \n",
    "    # Create a boolean array for the entire data, marking NaNs as outliers\n",
    "    outliers = np.full(data_array.shape, True)  # Default to True for NaN values\n",
    "    outliers[~nan_mask] = outliers_non_nan\n",
    "    \n",
    "    return outliers.tolist()\n",
    "\n",
    "def replace_outliers_zscore(\n",
    "    data: Union[List[float], np.ndarray, pd.Series], \n",
    "    outliers: List[bool], \n",
    "    method: str = \"median\"\n",
    ") -> Union[np.ndarray, pd.Series]:\n",
    "    \"\"\"\n",
    "    Replace outliers in a dataset based on Z-score detection with the mean or median of non-outlier values.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : Union[List[float], np.ndarray, pd.Series]\n",
    "        The input data containing original values.\n",
    "    outliers : List[bool]\n",
    "        A list of booleans indicating whether each value is an outlier (True) or not (False).\n",
    "    method : str, optional\n",
    "        The method to replace outliers, either 'median' or 'mean'. Default is 'median'.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    Union[np.ndarray, pd.Series]\n",
    "        The dataset with outliers replaced by the specified method's value.\n",
    "    \"\"\"\n",
    "    # Convert data to a numpy array for consistent processing\n",
    "    data_array = np.asarray(data)\n",
    "    \n",
    "    # Validate the method parameter\n",
    "    if method not in {\"median\", \"mean\"}:\n",
    "        raise ValueError(\"Invalid method. Use 'median' or 'mean'.\")\n",
    "    \n",
    "    # Compute the replacement value based on the specified method\n",
    "    non_outliers = data_array[~np.array(outliers)]\n",
    "    replacement_value = np.median(non_outliers) if method == \"median\" else np.mean(non_outliers)\n",
    "    \n",
    "    # Replace outliers with the calculated value\n",
    "    data_array[outliers] = replacement_value\n",
    "    \n",
    "    # Return the modified data in the same type as the input\n",
    "    if isinstance(data, pd.Series):\n",
    "        return pd.Series(data_array, index=data.index)\n",
    "    elif isinstance(data, list):\n",
    "        return data_array.tolist()\n",
    "    return data_array\n",
    "\n",
    "def identify_clean_outliers(\n",
    "    df: pd.DataFrame, \n",
    "    hrv_variable_name: str = \"HRV_RMSSD\", \n",
    "    method: str = \"median\", \n",
    "    threshold_z_score: float = 1.96\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies outliers in a specified HRV variable using the IQR method and replaces them with \n",
    "    the median or mean of the non-outlier values.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing the data to process. \n",
    "        Must include 'segment_name' and the specified `hrv_variable_name`.\n",
    "    hrv_variable_name : str, optional\n",
    "        The name of the HRV variable to process. Default is \"HRV_RMSSD\".\n",
    "    method : str, optional\n",
    "        The method to replace outliers, either \"median\" or \"mean\". Default is \"median\".\n",
    "    threshold_z_score : float, optional\n",
    "        The Z-score threshold to determine outliers. Default is 1.96 (i.e., >95%).\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The original DataFrame with two additional columns:\n",
    "        - `{hrv_variable_name}_outlier`: Boolean column indicating outliers.\n",
    "        - `{hrv_variable_name}_cleaned`: Column with outliers replaced by the specified method.\n",
    "    \"\"\"\n",
    "    # Ensure method is valid\n",
    "    if method not in {\"median\", \"mean\"}:\n",
    "        raise ValueError(\"Invalid method. Use 'median' or 'mean'.\")\n",
    "\n",
    "    for segment in df['segment_name'].unique():\n",
    "        subset_df = df[df['segment_name'] == segment].copy()\n",
    "\n",
    "        rmssd_values = subset_df[hrv_variable_name]\n",
    "        \n",
    "        # Identify outliers\n",
    "        subset_df[f'{hrv_variable_name}_outlier'] = identify_outliers_zscore(rmssd_values, threshold=threshold_z_score)\n",
    "        \n",
    "        # Replace outliers\n",
    "        subset_df[f'{hrv_variable_name}_cleaned'] = replace_outliers_zscore(\n",
    "            rmssd_values, \n",
    "            subset_df[f'{hrv_variable_name}_outlier'], \n",
    "            method=method\n",
    "        )\n",
    "\n",
    "        # Update the original DataFrame\n",
    "        df.loc[df['segment_name'] == segment, f'{hrv_variable_name}_outlier'] = subset_df[f'{hrv_variable_name}_outlier'].values\n",
    "        df.loc[df['segment_name'] == segment, f'{hrv_variable_name}_cleaned'] = subset_df[f'{hrv_variable_name}_cleaned'].values\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_segment_level_outliers(\n",
    "    df: pd.DataFrame, \n",
    "    hrv_variable_name: str = \"HRV_RMSSD\", \n",
    "    cv_threshold: float = 0.75\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detects segment-level outliers by checking the coefficient of variation (CV) for each segment.\n",
    "    Also flags segments as outliers if the standard deviation is zero (indicating no variability).\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing the data to process.\n",
    "    hrv_variable_name : str\n",
    "        The name of the HRV variable to process.\n",
    "    cv_threshold : float, optional\n",
    "        Threshold for the coefficient of variation. Segments with CV greater than this will be flagged.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Original DataFrame with an additional column 'segment_outlier' indicating segment-level outliers.\n",
    "    \"\"\"\n",
    "    segment_outliers = []\n",
    "    \n",
    "    for segment in df['segment_name'].unique():\n",
    "        segment_data = df[df['segment_name'] == segment][hrv_variable_name]\n",
    "        \n",
    "        # Calculate mean and standard deviation\n",
    "        mean = segment_data.mean()\n",
    "        std = segment_data.std()\n",
    "        \n",
    "        # Calculate Coefficient of Variation\n",
    "        cv = std / mean if mean != 0 else np.inf\n",
    "        \n",
    "        # Determine if the segment is an outlier\n",
    "        is_outlier = (cv > cv_threshold) or (std == 0)\n",
    "        segment_outliers.extend([is_outlier] * len(segment_data))\n",
    "    \n",
    "    df['segment_outlier'] = segment_outliers\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all non-empty directories in the processed data directory since they contain (hopefully) the HRV metrics\n",
    "all_items = PROCESSED_DATA_DIR.glob(\"*/\")\n",
    "all_dirs = [x for x in all_items if x.is_dir()]\n",
    "non_empty_dirs = [dir for dir in all_dirs if any(dir.iterdir())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all HRV Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HRV_RMSSD_plausible'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/neuroprofile/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HRV_RMSSD_plausible'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(file)\n\u001b[1;32m      9\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m plausible_to_nan(df, column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHRV_RMSSD\u001b[39m\u001b[38;5;124m\"\u001b[39m , lower_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, upper_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m130\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43midentify_clean_outliers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_cleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_z_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhrv_variable_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHRV_RMSSD_plausible\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m identify_clean_outliers(df, threshold_z_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.96\u001b[39m, hrv_variable_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHRV_RMSSD_cleaned\u001b[39m\u001b[38;5;124m'\u001b[39m, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# second round\u001b[39;00m\n\u001b[1;32m     12\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m detect_segment_level_outliers(df_cleaned, cv_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, hrv_variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHRV_RMSSD_cleaned_cleaned\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 153\u001b[0m, in \u001b[0;36midentify_clean_outliers\u001b[0;34m(df, hrv_variable_name, method, threshold_z_score)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegment_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m    151\u001b[0m     subset_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegment_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m segment]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 153\u001b[0m     rmssd_values \u001b[38;5;241m=\u001b[39m \u001b[43msubset_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhrv_variable_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Identify outliers\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     subset_df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhrv_variable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_outlier\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m identify_outliers_zscore(rmssd_values, threshold\u001b[38;5;241m=\u001b[39mthreshold_z_score)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroprofile/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroprofile/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HRV_RMSSD_plausible'"
     ]
    }
   ],
   "source": [
    "# In each directory, look for xlsx files, read them, and concatenate them into a single df\n",
    "n_files = 0 # to keep track of the number of excel files we read and process\n",
    "hrv_df = pd.DataFrame()\n",
    "for directory in non_empty_dirs:\n",
    "    xlsx_files = [file for file in directory.glob('*.xlsx') if not file.name.startswith('~$')]\n",
    "    n_files += len(xlsx_files)\n",
    "    for file in xlsx_files:\n",
    "        df = pd.read_excel(file)\n",
    "        df_cleaned = plausible_to_nan(df, column = \"HRV_RMSSD\" , lower_bound=9, upper_bound=130)\n",
    "        df_cleaned = identify_clean_outliers(df_cleaned, threshold_z_score=1.96, hrv_variable_name = 'HRV_RMSSD_plausible', method = \"mean\")\n",
    "        df_cleaned = identify_clean_outliers(df, threshold_z_score=1.96, hrv_variable_name = 'HRV_RMSSD_cleaned', method = \"mean\") # second round\n",
    "        df_cleaned = detect_segment_level_outliers(df_cleaned, cv_threshold=0.7, hrv_variable_name='HRV_RMSSD_cleaned_cleaned')\n",
    "        hrv_df = pd.concat([hrv_df, df_cleaned])\n",
    "\n",
    "# Export\n",
    "hrv_df.to_excel(HRV_DATA_DIR / 'cleaned_hrv_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semgnet-level detection should return outlier if all values are the same as well (e.g., NaNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plausible_df = plausible_to_nan(df, column = \"HRV_RMSSD\" , lower_bound=9, upper_bound=30)\n",
    "\n",
    "outliers = identify_outliers_zscore(plausible_df['HRV_RMSSD'], threshold=1.96)\n",
    "cleaned = replace_outliers_zscore(plausible_df['HRV_RMSSD'], outliers, method='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_clean_outliers(plausible_df, method=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroprofile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
