{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge HRV Metrics\n",
    "\n",
    "Purpose of this notebook is to:\n",
    "1. Read all the HRV Excel sheets from the output of the Neurokit2 pipeline\n",
    "2. Flagging all HRV values above and below an upper and lower bound as non-plausible.\n",
    "3. Perform outlier detection separately for each subject's condition on the HRV_RMSSD values using z-scores. For example, if a condition such as *baseline start* is a 300s recording with 30s HRV analysis windows, then outliers will be flagged across the 10 segments only relying on information from those 10 segments.\n",
    "4. Imputing outliers with the mean HRV value across the remaining non-outlier segments.\n",
    "5. Calculating the Coefficient of Variation (CoV) across the HRV values within a subject's condition (also using the imputed values). An upper bound on the CoV is used to flag entire conditions (i.e., all 10 segments) within a subject as outlier.\n",
    "6. After the outlier flagging / imputation, all the excel files are concatenated into a single file and exported as Excel sheet that can be used for further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import Union, List\n",
    "import sys\n",
    "sys.path.append(str(Path().cwd().parent/'src'))\n",
    "import utils.clean_impute_hrv as clean_impute_hrv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = Path().cwd()\n",
    "ROOT_DIR = WORKING_DIR.parent\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "HRV_DATA_DIR = DATA_DIR / 'hrv' # the output directory for the concatenated hrv data\n",
    "HRV_DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "REPORTS_DIR = ROOT_DIR / 'reports'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plausible_hrv_lower_upper_limit = [3, 120]\n",
    "threshold_z_score = 2.36 # was 1.96\n",
    "coeff_variation_upper_bound = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all non-empty directories in the processed data directory since they contain (hopefully) the HRV metrics\n",
    "all_items = PROCESSED_DATA_DIR.glob(\"*/\")\n",
    "all_dirs = [x for x in all_items if x.is_dir()]\n",
    "non_empty_dirs = [dir for dir in all_dirs if any(dir.iterdir())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, clean/impute, concatenate, and export the HRV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each directory, look for xlsx files, read them, and concatenate them into a single df\n",
    "n_files = 0 # to keep track of the number of excel files we read and process\n",
    "hrv_df = pd.DataFrame()\n",
    "for directory in non_empty_dirs:\n",
    "    xlsx_files = [file for file in directory.glob('*.xlsx') if not file.name.startswith('~$')]\n",
    "    n_files += len(xlsx_files)\n",
    "    for file in xlsx_files:\n",
    "        df = pd.read_excel(file)\n",
    "        df_cleaned = clean_impute_hrv.plausible_to_nan(df, column = \"HRV_RMSSD\" , lower_bound=plausible_hrv_lower_upper_limit[0]\n",
    "                                      , upper_bound=plausible_hrv_lower_upper_limit[1])\n",
    "        df_cleaned = clean_impute_hrv.identify_clean_outliers(\n",
    "            df_cleaned, \n",
    "            threshold_z_score=threshold_z_score, \n",
    "            hrv_variable_name = 'HRV_RMSSD_plausible', \n",
    "            method = \"mean\")\n",
    "        df_cleaned = clean_impute_hrv.detect_segment_level_outliers(df_cleaned, cv_threshold=coeff_variation_upper_bound, hrv_variable_name='HRV_RMSSD_plausible_imputed')\n",
    "        hrv_df = pd.concat([hrv_df, df_cleaned])\n",
    "\n",
    "# # Export\n",
    "hrv_df.to_excel(HRV_DATA_DIR / 'cleaned_hrv_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroprofile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
