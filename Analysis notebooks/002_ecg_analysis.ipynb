{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Analysis\n",
    "\n",
    "### Purpose\n",
    "Purpose of this notebook is to preprocess and analyse the Mindware ECG data\n",
    "\n",
    "### Approach\n",
    "1. HRV data will be calculated per 30s analysis-window per segment (baseline, story 1, story 2,...)\n",
    "2. RSA values will be calculate once per segment because longer segments are needed to calculate PB-RSA.\n",
    "3. Aggregation for HRV values happens at the analysis-window to arrive at average 30s HRV per segment\n",
    "4. No aggregation for RSA will be performed since RSA already is at the segment level\n",
    "\n",
    "#### Notes\n",
    "- Outlier detection will be performed for HRV values at the level of analysis-windows usign z-score approach. Moreover, only analysis windows with > 20 peaks will be used for aggregation and outlier detection \n",
    "\n",
    "### Input / Output\n",
    "\n",
    "- Input `~/data/interim/signals`\n",
    "- Processed data output (including metrics): `~/data/processed/ecg`\n",
    "- QA figures Output: `~reports/QA/ecg`\n",
    "- Aggregated metrics `~data/final/ecg`\n",
    "\n",
    "## TO-DO's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'app.ecg_high_level_fnc' from '/Users/lukasspiess/Library/CloudStorage/OneDrive-SpiessSolution/Neurophysiological profiles/General/Mindware data analysis/src/app/ecg_high_level_fnc.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fmt: off\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Union, List, Dict, Tuple\n",
    "import warnings\n",
    "import pandas as pd \n",
    "import importlib\n",
    "import traceback\n",
    "sys.path.append(str(Path().cwd().parent/\"src\"))\n",
    "sys.path.append(str(Path().cwd().parent/\"app\"))\n",
    "import ecg_utils.data_utils as data_utils\n",
    "import ecg_utils.parameters as parameters\n",
    "import ecg_utils.clean_impute as clean_impute\n",
    "import ecg_utils.nk_pipeline as nk_pipeline\n",
    "import ecg_utils.common as common\n",
    "import app.ecg_high_level_fnc as app\n",
    "import numpy as np\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(parameters)\n",
    "importlib.reload(common)\n",
    "importlib.reload(nk_pipeline)\n",
    "importlib.reload(clean_impute)\n",
    "importlib.reload(app)\n",
    "# fmt:on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = Path().cwd()\n",
    "ROOT_DIR = WORKING_DIR.parent\n",
    "\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "INTERIM_SIGNAL_DATA_DIR = DATA_DIR / 'interim' / 'signals'\n",
    "\n",
    "PROCESSED_ECG_DATA_DIR = DATA_DIR / 'processed' / 'ecg'\n",
    "PROCESSED_ECG_DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "FINAL_ECG_METRICS_DIR = DATA_DIR / 'final' / 'ecg_metrics'\n",
    "FINAL_ECG_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "REPORTS_DIR = ROOT_DIR / 'reports'\n",
    "QA_REPORTS_DIR = REPORTS_DIR / 'QA' / 'ecg'\n",
    "QA_REPORTS_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folders_in_directory(directory_path: Union[str,Path]) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Get all folders in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path of the directory to scan for folders.\n",
    "\n",
    "    Returns:\n",
    "        list[Path]: A list of Path objects representing folders in the directory.\n",
    "    \"\"\"\n",
    "    path = Path(directory_path)\n",
    "    return [item for item in path.iterdir() if item.is_dir()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 88 data files\n"
     ]
    }
   ],
   "source": [
    "# data files\n",
    "data_files_generator = INTERIM_SIGNAL_DATA_DIR.glob('*.csv')\n",
    "print(f\"Identified {data_files_generator.__sizeof__()} data files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse ECG\n",
    "\n",
    "**For each subject:**\n",
    "1. Read data\n",
    "2. preprocess\n",
    "3. segment\n",
    "4. calculate windowed HRV + generate peak QA plots + output hrv metrics + output cleaned signal data. \n",
    "5. calculate and export RSA metrics per segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_pipeline_params = parameters.base_params\n",
    "not_successful = list()\n",
    "for index, signal_filepath in enumerate(data_files_generator):\n",
    "    \n",
    "    try:\n",
    "        # prepare data\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            signal_df = pd.read_csv(signal_filepath)\n",
    "        subject_id = str(signal_df['subject_id'].unique()[0])\n",
    "        # if subject_id != \"2094\":\n",
    "        #     continue\n",
    "        ecg_series = signal_df['MWMOBILEJ_Bio']\n",
    "        print(f\"Processing subject {subject_id}\")\n",
    "\n",
    "        subject_figure_output_dir = QA_REPORTS_DIR / subject_id \n",
    "        subject_figure_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        subject_data_output_dir = PROCESSED_ECG_DATA_DIR / subject_id\n",
    "        subject_data_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # preprocess and join with event data\n",
    "        tmp_signals_preproc_df = nk_pipeline.ecg_preprocess(ecg_series, ecg_pipeline_params)\n",
    "        preproc_df = signal_df.merge(tmp_signals_preproc_df, left_index=True, right_index=True)\n",
    "        \n",
    "        # segment\n",
    "        segments_df_list = data_utils.segment_df(preproc_df, ecg_pipeline_params)\n",
    "        data_utils.check_segment_list(segments_df_list)\n",
    "        [df.set_index('time_seconds_original_file', inplace=True) for df in segments_df_list] # to have time in ms as index\n",
    "        \n",
    "        # calculate and export HRV \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            hrv_df, ecg_df = app.compute_windowed_hrv_across_segments(\n",
    "                segments_df_list=segments_df_list,\n",
    "                parameters=ecg_pipeline_params,\n",
    "                figure_output_dir=subject_figure_output_dir,\n",
    "                data_output_dir=subject_data_output_dir,\n",
    "                subject_id = subject_id,\n",
    "                create_qa_plots=True\n",
    "            )\n",
    "        \n",
    "\n",
    "        # Calculate and export RSA metrics\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            rsa_agg_df = nk_pipeline.calculate_rsa_per_segment(segments_df_list, \n",
    "                                                            ecg_pipeline_params, \n",
    "                                                            subject_id=subject_id, \n",
    "                                                            data_output_dir=subject_data_output_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {signal_filepath}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        not_successful.append(signal_filepath)\n",
    "        break\n",
    "    \n",
    "    \n",
    "\n",
    "print(f\"Processing done. Not successful for {len(not_successful)} files: {not_successful}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the HRV and RSA data, aggregate, and export as group level file\n",
    "\n",
    "1. Read HRV an RSA data of a subject\n",
    "2. Perform outlier detection/correction on HRV an RSA values\n",
    "3. Aggregate HRV data (to have a single value per segment)\n",
    "4. Concatenate the data of each subject into a single file and export for statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_subject_folders = get_folders_in_directory(PROCESSED_ECG_DATA_DIR)\n",
    "group_level_metrics_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each subject\n",
    "for subject_folder in processed_subject_folders:\n",
    "    subject_id = subject_folder.parts[-1]\n",
    "    # print(f\"Processing subject {subject_id}\")\n",
    "    \n",
    "    # Read data\n",
    "    hrv_df = pd.read_excel(subject_folder / 'hrv_metrics.xlsx').drop(columns='Unnamed: 0')\n",
    "    rsa_df = pd.read_excel(subject_folder / 'rsa_metrics.xlsx').drop(columns='Unnamed: 0')\n",
    "    \n",
    "    # Merge the two metrics\n",
    "    metrics_df = hrv_df.merge(rsa_df, on='segment_name', how = 'left', suffixes=('_hrv', '_rsa'))\n",
    "    metrics_df = clean_impute.flag_windows_insufficient_n_peaks(metrics_df, min_peaks_required=20)\n",
    "    \n",
    "    # HRV outlier detection\n",
    "    metrics_df = clean_impute.flag_outliers_based_on_zscore(metrics_df, 'HRV_SDNN', z_threshold=2.56)\n",
    "    metrics_df = clean_impute.flag_usable_aggregation_windows(metrics_df)\n",
    "    \n",
    "    # provide information about the number of usable analysis windows for each segment and add that as column to the dataframe\n",
    "    usable_windows_per_segment = metrics_df.groupby('segment_name')['usable_window'].sum()\n",
    "    metrics_df = metrics_df.assign(usable_analysis_windows_in_segment = metrics_df['segment_name'].map(usable_windows_per_segment))\n",
    "    \n",
    "    # Aggregate HRV values \n",
    "    single_subject_metrics_agg_df = metrics_df.groupby('segment_name').mean()\n",
    "    \n",
    "    # clean up redundant columns etc.\n",
    "    single_subject_metrics_agg_df = single_subject_metrics_agg_df.rename(columns = {\n",
    "        \"subject_id_hrv\": \"subject_id\",\n",
    "        \"start_time_hrv\": \"start_time\",\n",
    "        \"end_time_hrv\": \"end_time\",\n",
    "    })\n",
    "    single_subject_metrics_agg_df = single_subject_metrics_agg_df.drop(columns = \n",
    "                                                                       [\"subject_id_rsa\", \"start_time_rsa\", \"end_time_rsa\",\n",
    "                                                                        \"usable_window\", \"window_has_enough_peaks\", \"HRV_SDNN_outlier\",\n",
    "                                                                        \"start_time\", \"end_time\", \"n_peaks_detected\",\n",
    "                                                                        \"analysis_window\", \"MWMOBILEJ_GSC\"\n",
    "                                                                        ], errors=\"ignore\")\n",
    "    \n",
    "    # concatenate with the data from other subjects\n",
    "    group_level_metrics_df = pd.concat([group_level_metrics_df, single_subject_metrics_agg_df])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform baseline correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare\n",
    "group_level_metrics_df = group_level_metrics_df.reset_index()\n",
    "baseline_df = group_level_metrics_df[group_level_metrics_df[\"segment_name\"] == \"Baseline\"].set_index(\"subject_id\")\n",
    "\n",
    "# Merge baseline values with the original DataFrame\n",
    "df = group_level_metrics_df.merge(\n",
    "    baseline_df[[\"RSA_PorgesBohrer\", \"HRV_SDNN\", \"heart_rate_bpm\"]],\n",
    "    on=\"subject_id\",\n",
    "    suffixes=(\"\", \"_baseline\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply baseline correction\n",
    "df[\"RSA_PorgesBohrer_corrected\"] = df[\"RSA_PorgesBohrer\"] - df[\"RSA_PorgesBohrer_baseline\"]\n",
    "df[\"HRV_SDNN_corrected\"] = df[\"HRV_SDNN\"] - df[\"HRV_SDNN_baseline\"]\n",
    "df[\"heart_rate_bpm_corrected\"] = df[\"heart_rate_bpm\"] - df[\"heart_rate_bpm_baseline\"]\n",
    "\n",
    "# Save the aggregated, baseline-corrected metrics\n",
    "baseline_corrected_metrics_df = df[df[\"segment_name\"] != \"Baseline\"]\n",
    "baseline_corrected_metrics_df.to_excel(FINAL_ECG_METRICS_DIR / 'group_level_blc_ecg_metrics.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroprofile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
